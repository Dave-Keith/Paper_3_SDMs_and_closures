<!-- Set the theme for the figures -->
```{r theme, echo=F, include=F, paged.print=FALSE,cache =F}
theme_set(theme_few(base_size = 12))
```

<!-- Bring in the data + figures   -->

```{r pred-data, echo=F, include=F, paged.print=FALSE,cache =T}

# Decide what you want "Hi probablity" to be for this analysis..
hi.prob <- 0.75


# Now I want to make the prediction field from the new prediction models
mod.names <- names(pred.dat)
n.mods <- length(mod.names)
pred.res <- NULL
pred.rf <- NULL
for(i in 1:n.mods)
{
  res <- pred.dat[[mod.names[i]]]
  rf <- rand.field.pred[[mod.names[[i]]]]
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_3)))
  {
    n.eras <- length(unique(res$years_5))
    eras <- factor.2.number(unique(res$years_5))
  } # End if loop
  
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_5)))
  {
    n.eras <- length(unique(res$years_3))
    eras <- factor.2.number(unique(res$years_3))
  } # end if loop
  
  res <- st_as_sf(res,coords = c("X","Y"), crs = st_crs(mesh.grid),remove = F)
  mesh.tmp <- st_sf(rep(st_sf(mesh.sf$vertices)$geometry,n.eras), crs = st_crs(mesh.grid))
  mesh.tmp$ID <- rf$ID <- 1:nrow(rf)
  rf$era <- sort(rep(eras,nrow(mesh.sf$vertices)))
  rf <- merge(mesh.tmp,rf, by = "ID")

# Set the colors un
  # Now for some reason my prediction grid doesn't quite line up with my prediciton mesh, so clip the mesh to match
  res <- st_join(mesh.grid,res)
  rf <- st_intersection(rf,clp.pred)
  for(n in min(eras):max(eras))
  {
    if(all(is.na(res$years_3)))
    {
    yrs <- paste0(substr(dat.final %>% filter(years_5 == n, survey == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                  substr(dat.final %>% dplyr::filter(years_5 == n, survey == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
    if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
    res$yrs[res$years_5==n] <- yrs
    rf$yrs[rf$era == n] <- yrs
    }
    
    if(all(is.na(res$years_5)))
    {
      yrs <- paste0(substr(dat.final %>% filter(years_3 == n, survey == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                    substr(dat.final %>% dplyr::filter(years_3 == n, survey == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
      if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
      res$yrs[res$years_3==n] <- yrs
      rf$yrs[rf$era == n] <- yrs
    }
    
  } # end   for(n in min(eras):max(eras))

  res <- res[!is.na(res$yrs),]
  rf <- rf[!is.na(rf$yrs),]
  # So calculating area is smart using that set units, though they are all idenitcal...
  res$area <- res %>% st_area() %>% set_units("km^2")
  res <- res %>% dplyr::filter(pred >= 0) # THIS IS SUPER IMPORTANT!!  WHAT IS THE probability we are looking at for this.
  pred.res[[mod.names[i]]] <- res
  pred.rf[[mod.names[i]]] <- rf
} # end for (i) loop

# This is the thing I need to make the prediction plots and also for the COG and area calculations.
pred.res <- do.call("rbind",pred.res) # This is a useful general purpose object I want
pred.rf <- do.call("rbind",pred.rf)

pred.res$species[pred.res$species == "yt_PA"] <- "Yellowtail"
pred.res$species[pred.res$species == "cod_PA"] <- "Cod"
pred.res$survey[pred.res$survey == "nmfs-spring"] <- "Spring"
pred.res$survey[pred.res$survey == "nmfs-fall"] <- "Fall"
pred.res$survey[pred.res$survey == "RV"] <- "Winter"
pred.res$survey <- factor(pred.res$survey, levels = c("Winter","Spring","Fall"))
# Now the randome field...
pred.rf$species[pred.rf$species == "yt_PA"] <- "Yellowtail"
pred.rf$species[pred.rf$species == "cod_PA"] <- "Cod"
pred.rf$survey[pred.rf$survey == "nmfs-spring"] <- "Spring"
pred.rf$survey[pred.rf$survey == "nmfs-fall"] <- "Fall"
pred.rf$survey[pred.rf$survey == "RV"] <- "Winter"
pred.rf$survey <- factor(pred.rf$survey, levels = c("Winter","Spring","Fall"))
```


```{r area-calcs, echo=F, include=F, paged.print=FALSE,cache =T}

# I want the EEZ shape file to split up Canada and US
# Figure out where your tempfiles are stored
temp <- tempfile()
# Download this to the temp directory you created above
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/EEZ/EEZ.zip", temp)
#$ Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# Now read in the shapefile
eez.all <- st_read(paste0(temp2, "/EEZ.shp"))
rm(temp,temp2)
clp.eez <- st_as_sf(data.frame(X = c(-70,-70,-62.2,-62.2),Y = c(39,44,44,39)),coords = c("X","Y"),crs = 4326)
clp.eez <- st_cast(st_combine(clp.eez),"POLYGON")
eez.all <- eez.all %>% st_transform(4326)
tmp <- st_intersection(eez.all,clp.eez)
eez.can <- concaveman(tmp)
eez.can <- eez.can %>% st_transform(32619)
# I think this is what I want for this, I think...
# Grab the coorindates for CA1, CA2, and the cod/yellowtail closures which are inside this other_boundaries zip file.
temp <- tempfile()
# Download this to there
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/other_boundaries/other_boundaries.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# Now grab the individual shape files I want.
CA1 <- st_read(dsn = paste0(temp2,"/CA1.shp"))
CA1 <- st_transform(CA1,crs = 32619)
CA2 <- st_read(dsn = paste0(temp2,"/CA2.shp"))
CA2 <- st_transform(CA2,crs = 32619)
yt.closures <- st_read(dsn =paste0(temp2,"/yt_closures.shp"))
yt.closures<- st_transform(yt.closures,crs=32619)
cod.closures <- st_read(dsn =paste0(temp2,"/cod_closures.shp"))
cod.closures <- st_transform(cod.closures,crs=32619)
rm(temp,temp2)
# Now grab the coordinates for the survey boundaries for GBa and gbb, inside of the survey_boundaries zip.
temp <- tempfile()
# Download this to there
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/survey_boundaries/survey_boundaries.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# And we get GBa and GBb from there
gba.surv <- st_read(dsn = paste0(temp2,"/GBa.shp"))
gbb.surv <- st_read(dsn = paste0(temp2,"/GBb.shp"))
# Make an 'all of gb' sf object
gb.surv <- st_union(gba.surv,gbb.surv)
# This removes holes, still have one small artifact out to the east, but it matters little...
gb.surv <- nngeo::st_remove_holes(gb.surv)
gb.surv <- st_transform(gb.surv,crs= 32619)
scal.tot.area <- st_area(gb.surv) %>% set_units("km^2")
# Put all the closures together so we can easily plot them.
all.closures <- c(st_geometry(CA1),st_geometry(CA2),st_geometry(yt.closures),st_geometry(cod.closures))

# The survey data, in sf form
dat.sf <- st_as_sf(dat.final, coords = c('lon','lat'),crs = 4326,remove=F)
dat.sf <- st_transform(dat.sf,crs = 32619)
##### Done with data loading... Set some variables for the rest of the show..

# Decide what you want "Hi probablity" to be for this analysis..
hi.prob <- 0.75


# #  A nice clean polygon of the core of the GB area we want to deal with here.
 clp.poly <- st_as_sf(data.frame(X = c(508000,508000,900000,650000,600000,550000),
                                 Y=c(4540000,4350000,4674000,4674000,4661000,4622000),ID=1),coords = c("X","Y"),crs= 32619)
 clp.poly <- st_cast(st_combine(clp.poly),"POLYGON")
# # Now use the bigger clp with this other clip to get a nice clipped GB area...
 clp.pred <- st_intersection(clp,clp.poly)
GB.pred.area <- clp.pred %>% st_area() %>% set_units("km^2") # So really about 42,000 km in our clip area, good.

can.pred.area <- st_intersection(clp.pred,eez.can) %>% st_area() %>% set_units("km^2")
us.pred.area <- st_difference(clp.pred,eez.can) %>% st_area() %>% set_units("km^2")



#####
# Data for Center of gravity of the species distributions has shifted
#####
# So here we need the predicted field for each model. I think we want to show how COG has moved for cod and yellowtail
# and have a panel for each survey, might end up being better as 2 figures, not sure yet.  But we'll want 6 cogs
# I also want to clean up the names in pred.dat (could come in handy throughout rather that using the labeller everywhere..

names.survs <- unique(pred.res$model)
pred.cog <- NULL

for(i in 1:length(names.survs))
{
  res <- pred.res %>% filter(model == names.survs[i])
  
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_3)))
  {
    n.eras <- length(unique(res$years_5))
    eras <- factor.2.number(unique(res$years_5))
  } # End if loop
  
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_5)))
  {
    n.eras <- length(unique(res$years_3))
    eras <- unique(res$years_3)
  } # end if loop
    
  # Make this into an sf object
 # res <- st_as_sf(res,coords = c("X","Y"), crs = st_crs(mesh.grid),remove = F)
  # Combine the mesh into the results so we have predictions at each mesh element
  #res <- st_join(mesh.grid,res)  
  # Get the years right for each input.
 for(n in min(eras):max(eras))
  {
    if(all(is.na(res$years_3)))
    {
    yrs <- paste0(substr(dat.final %>% filter(years_5 == n, surveys == as.character(unique(res$survey))) %>% dplyr::summarise(min = min(year)),3,4),"-",
                  substr(dat.final %>% dplyr::filter(years_5 == n, surveys == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
    if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
    res$yrs[res$years_5==n] <- yrs
    }
    
    if(all(is.na(res$years_5)))
    {
      yrs <- paste0(substr(dat.final %>% filter(years_3 == n, surveys == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                    substr(dat.final %>% dplyr::filter(years_3 == n, surveys == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
      if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
      res$yrs[res$years_3==n] <- yrs
    }
    
  } # end   for(n in min(eras):max(eras))

  res <- res[!is.na(res$yrs),]
  # So calculating area is smart using that set units, though they are all idenitcal...
  res$area <- res %>% st_area() %>% set_units("km^2")
  res <- res %>% filter(pred >= hi.prob) # THIS IS SUPER IMPORTANT!!  WHAT IS THE probability we are looking at for this.

  cog_n_area <- as.data.table(res)[,cog.calc(X,Y,pred), by = yrs]
  cog_n_area <- cog_n_area[order(cog_n_area$yrs)]
  cog_n_area <- st_as_sf(cog_n_area,coords = c('x','y'), crs= st_crs(mesh.grid), remove=F)
  area <- res %>% group_by(yrs) %>% dplyr::summarise(area = sum(area))
  st_geometry(area) <- NULL
  cog_n_area$area <- area$area
  # This object has what we need for COG and area calcs.  
  cog_n_area$mod <- names.survs[i]
  cog_n_area$species <- res$species[1]
  cog_n_area$survey <- res$survey[1]
  pred.cog[[names.survs[i]]] <- cog_n_area
}


cog.n.area <- do.call('rbind',pred.cog)
# Make names nice...=

#cog.n.area$eras <- as.numeric(factor(cog.n.area$yrs, labels =1:length(unique(cog.n.area$yrs))))
```

```{r combo-pred-rf, echo=F, include=F,cache=T}

# Some messing about with standard deviaiotns, second order indicators.... hmmm

pred.res$year_era <- as.numeric(substr(pred.res$yrs,1,4))
# How does sd change over time....
pred.mn.sd <- pred.res %>% group_by(species,year_era,survey) %>% dplyr::summarise(mn.sd = mean(pred.sd))
rf.mn.sd <- pred.rf %>% group_by(species,era,survey) %>% dplyr::summarise(mn.sd = mean(sd))
# How about we intersect the pred.res with the random field and compare the predictions against the random field...
# Don't do this, it takes like 20 minutes!
# tst <- st_intersection(pred.rf,pred.res)
# tst


# So to combine the rf and pred fields without creating 13,000,000 duplicates we need to do this by survey/species/era...
#surveys <- unique(pred.res$survey)
surveys <- unique(pred.rf$survey) 
n.surveys <- length(surveys)
species <-unique(pred.rf$species)
n.species <- length(species)

prf.tmp <- NULL
prf2.tmp <- NULL
for(i in 1:n.surveys)
{
  for(j in 1:n.species)
  {
    spec <- species[j]
    surv <- surveys[i]
    pred.tmp <- pred.res %>% dplyr::filter(survey == surv, species  == spec)
    rf.tmp <- pred.rf %>% dplyr::filter(survey == surv,species == spec)
    eras <- unique(pred.tmp$year_era)
    n.eras <- length(eras)
    for(k in 1:n.eras)
    {
      er <- eras[k]
      pred.tmp2 <- pred.tmp %>% filter(year_era == er)
      rf.min.era <- min(rf.tmp$era)
      rf.tmp2 <- rf.tmp %>% filter(era == (rf.min.era + k -1))
      tmp <- st_intersection(pred.tmp2,rf.tmp2)
      prf.tmp[[as.character(k)]] <- tmp
    }
    prf2.tmp[[paste0(surveys[i],"_",species[j])]] <- do.call("rbind",prf.tmp)
  }
}

pred.and.rf.fields <- do.call("rbind",prf2.tmp)
pred.and.rf.fields$survey <- factor(pred.and.rf.fields$survey, levels = c("Winter","Spring","Fall"))

cols <- addalpha(c("red","blue","grey"),1)
p.pred.mn <- ggplot(pred.mn.sd) + geom_line(aes(x = year_era,y = mn.sdsurvey)) + facet_wrap(~species)  + scale_color_manual(values = cols) + theme_few()
p.rf.mn <- ggplot(rf.mn.sd) + geom_line(aes(x = era,y = mn.sd,color= survey)) + facet_wrap(~species) + scale_color_manual(values = cols) + theme_few()

# Plotting predictions against their standard deviation.  Note that sd peaks when P(E) is near 0.5, so need to standardize by P(E)
# to identify areas where sd is higher than expectation.
cols <- addalpha(c("red","blue","grey"),0.2)
p.sd <- ggplot(pred.res) + geom_point(aes(x=pred.sd,y=pred,color=survey),size=0.25) + theme_few() + facet_wrap(~species+ year_era) + 
  ylab("Probability of encounter") + xlab("Standard Deviation of P(encounter)") + scale_color_manual(values = cols) 
save_plot(paste0(direct.proj,"Results/Figures/PE_vs_sd(PE).png"),p.sd,base_width =11,base_height =8,units='in')

# Not really interesting I don't think...
# rf.sd <- ggplot(pred.rf) + geom_point(aes(x=sd,y=mean,color=survey),size=0.25) + theme_few() + facet_wrap(~species+era) + ylab("Mean of Random Field") + 
#   xlab("Standard Deviation of Random Field") + scale_color_manual(values = cols) 
# save_plot(paste0(direct.proj,"Results/Figures/RF_vs_sd(RF).png"),rf.sd,base_width =11,base_height =8,units='in')
# 
# rf.pred.sd <- ggplot(pred.and.rf.fields) + geom_point(aes(x=sd,y=pred,color = survey),size=0.25) + facet_wrap(~species+year_era)+ theme_few() +  
#    ylab("Probability of encounter") + xlab("Standard Deviation of Random Field") + scale_color_manual(values = cols) 
#  save_plot(paste0(direct.proj,"Results/Figures/Pred_vs_sd(RF).png"),rf.pred.sd,base_width =11,base_height =8,units='in')

 
 ## So I don't think the sd from the random field is interesteing, but a modified version of the sd of th prediction field might me.  
 ## I think we want to bin the sd by prediciion (at 1% increments) and identify if the sd is above or below median for the sd bin... 
 ## something like
 
pred.res$pred.bin <- round(pred.res$pred,digits =2)
hmm <- data.frame(pred.res) %>% dplyr::group_by(pred.bin,survey,species) %>% dplyr::summarize(mn.sd.bin = mean(pred.sd))
pred.res <- left_join(pred.res,hmm,by=c("pred.bin","survey","species"))
pred.res <- pred.res %>% dplyr::mutate(sd.dist = pred.sd-mn.sd.bin, stan.sd.dist = (pred.sd-mn.sd.bin)/mn.sd.bin)
# Do the same thing with the random field intersection object...
pred.and.rf.fields$pred.bin <- round(pred.and.rf.fields$pred,digits =2)
hmm <- data.frame(pred.and.rf.fields) %>% dplyr::group_by(pred.bin,survey,species) %>% dplyr::summarize(mn.sd.bin = mean(pred.sd))
pred.and.rf.fields <- left_join(pred.and.rf.fields,hmm,by=c("pred.bin","survey","species"))
pred.and.rf.fields <- pred.and.rf.fields %>% dplyr::mutate(sd.dist = pred.sd-mn.sd.bin, stan.sd.dist = (pred.sd-mn.sd.bin)/mn.sd.bin)


# pred.vs.dev.sd <- ggplot(pred.res) + geom_point(aes(x=stan.sd.dist,y=pred,color = survey),size=0.25) + facet_wrap(~species+year_era)+ theme_few() +  
#    ylab("Probability of encounter") + xlab("Deviation from standard deviation") + scale_color_manual(values = cols) 
#  save_plot(paste0(direct.proj,"Results/Figures/Pred_vs_deviation(sd).png"),pred.vs.dev.sd,base_width =11,base_height =8,units='in')
# 
# # What is the relationship between the Random field residual and our new metric, are we just recreating the RF uncertainty or is there something else in here...
# # Not seeing any real realtionship in general, 
# rf.vs.dev.sd <- ggplot(pred.and.rf.fields) + geom_point(aes(x=stan.sd.dist,y=mean,color = survey),size=0.25) + facet_wrap(~species+year_era)+ theme_few() +  
#    ylab("Random Field ") + xlab("Deviation from standard deviation") + scale_color_manual(values = cols) 
# save_plot(paste0(direct.proj,"Results/Figures/RF_vs_deviation(sd).png"),rf.vs.dev.sd,base_width =11,base_height =8,units='in')
# # Now gonna look at RF vs the prediction SD...
# rf.vs.pred.sd <- ggplot(pred.and.rf.fields) + geom_point(aes(x=pred.sd,y=mean,color = survey),size=0.25) + facet_wrap(~species+year_era)+ theme_few() +  
#    ylab("Random Field ") + xlab("Prediction standard deviation") + scale_color_manual(values = cols) 
# save_plot(paste0(direct.proj,"Results/Figures/RF_vs_SD(Pred).png"),rf.vs.pred.sd,base_width =11,base_height =8,units='in')

# Looking at our metric spatially, seems like it is reasonable enough to move forward. 
# hgt <- unit(5,'cm')
# brk <- seq(-0.6,1.7,by=0.2)
# lims <- range(brk)
# col <- addalpha(pals::viridis(101),1)
# sf <- scale_fill_gradientn(colours = col, limits=lims,breaks=brk,name=paste0(en2fr("Deviation from sd",french,case='sentence',custom = rosetta_terms)))
# sc <- scale_colour_gradientn(colours = col, limits=lims,breaks=brk,name=paste0(en2fr("Deviation from sd",french,case='sentence',custom = rosetta_terms)))
# 
# tst <- pred.res %>% dplyr::filter(survey == "Spring",species == "Cod")
# summary(tst$stan.sd.dist)
#                                   
# plt.dev.sd.tst <- bp + geom_sf(data = pred.res %>% dplyr::filter(survey == "Spring",species == "Cod")  ,aes(fill = stan.sd.dist,colour=stan.sd.dist)) +
# facet_wrap(~yrs,ncol=6) +
#   coord_sf(datum=32619) + sf + sc  +# theme_map() +
#  theme(legend.key.width =hgt,text = element_text(size=16), legend.position="top")
#  save_plot(paste0(direct.proj,"Results/Figures/deviation_spatial_plot.png"),plt.dev.sd.tst,base_width =15,base_height =8,units='in')
#  
```


```{r intersect-pe-and-sd-closures, echo=F, include=F,cache=T}
# So the primary story has to be changes in EP/core area within the closures.  But we could add a secondary indicator story using the SDs, an sd of the standardized SD's if 
# you will, while the numbers are meaningless, you wouldn't want to see this number increasing over time and you could look spatially at large closures to see where you have
# less certainty, if this happens to be an area that you are particularly interested in you probably should invest money in these areas to improve your knowledge.  Need to understand any relationship between estimate and standard deviation when doing this of course.

# Now get the area in total for Canada and the US, not sure I'll need them...
us.grid <- st_difference(mesh.grid,eez.can)
can.grid <- st_intersection(mesh.grid,eez.can)
can.area <- st_area(can.grid) %>% set_units("km^2") %>% dplyr::as_tibble() %>% dplyr::summarise(area = sum(value))
us.area <- st_area(us.grid) %>% set_units("km^2") %>% dplyr::as_tibble() %>% dplyr::summarise(area = sum(value))
# I think here I'm going to want to loop through each closure
# Stitch together all the closed area objects into one list
areas <- list(`Groundfish closure` = can.grid,
              `Closed Area 1` = CA1,
              `Closed Area 2` = CA2,
              `Yellowtail closure` = yt.closures,
              `Cod closure` = cod.closures,
              Canada =  can.grid,
              US = us.grid,
              `Georges Bank` = mesh.grid)
n.areas <- length(areas)

#Here's our workhorse, from the dashboard with modifications for SD....

surveys <-   unique(pred.res$survey)
n.survyes <- length(surveys)
specs <- unique(pred.res$species)
n.specs <- length(specs)

#cls.prob <- NULL    
#cls.sd <- NULL
#clos.sd.summary <- NULL
#clos.area.summary <- NULL
area.summary <- NULL
sd.summary <- NULL
for(i in 1:n.surveys)
{
  for(j in 1:n.specs)
  {
    
    surv <- surveys[i]
    spec <- specs[j]
    pred.sub <- pred.res %>% dplyr::filter(survey == surv,species == spec)
    # Going to reorder this by year, just a bit cleaner when looking at data later....
    pred.sub <- pred.sub[order(pred.sub$yrs),]
    
    # So this object is the preds filtered to the value we want to pick as our core area value...
    preds.prob <- pred.sub %>% dplyr::filter(pred >= hi.prob)
    
    for(k in 1:n.areas)
    {
      # First we subset the data as necessary, then we "fill' up the data so we have information every year, even if
      # that information is redundant information (i.e. the RF hasn't changed).
      nm <-  names(areas)[k]
      area <- areas[[k]]
      if(nm %in% c("Groundfish closure","Canada","US","Georges Bank"))
      {
        area <- st_cast(area,"POINT")
        area <- concaveman(area) 
      } # end concaveman-ism
      
      # Pick this for each loop through the k's as I need to overwrite it for the closures...
      all.years <- data.frame(yrs = unique(pred.sub$yrs)) 
      first.years <- as.numeric(substr(all.years$yrs,1,4))
      # Now get the two subsets I'm interested in.
      if(!nm %in% c("Yellowtail closure","Cod closure")) 
      {
        prob.tmp <- st_intersection(preds.prob,area)
        sds.tmp <- st_intersection(pred.sub,area)
        
        if(spec == "Cod")
        {
          # So this gets the
          if(surv == "Winter") 
          {
            prob.first <-   prob.tmp %>% dplyr::filter(yrs == "1987-89") 
            prob.tmp   <-   prob.tmp %>% dplyr::filter(yrs != "1987-89")
            sds.first  <-   sds.tmp %>% dplyr::filter(yrs == "1987-89") 
            sds.tmp    <-   sds.tmp %>% dplyr::filter(yrs != "1987-89")
          } # end if surv == "winter" statement
          
          tmp <- NULL
          tmp2 <- NULL
          tmp.sd <- NULL
          tmp2.sd <- NULL
          
          for(p in 0:4) 
          {
            tmp[[p+1]] <- prob.tmp %>% dplyr::mutate(year = as.numeric(first_year) + p)
            tmp.sd[[p+1]] <- sds.tmp %>% dplyr::mutate(year = as.numeric(first_year) + p)
            if(surv == "Winter" & p < 3)
            {
              tmp2[[p+1]] <- prob.first %>% dplyr::mutate(year = as.numeric(first_year) + p) # Only needed for the Winter survey
              tmp2.sd[[p+1]] <- sds.first %>% dplyr::mutate(year = as.numeric(first_year) + p) # Only needed for the Winter survey
            } # end if(surv == "Winter" & p < 3)
          } # end for p loop
          
          # Unwrap objects...
          prob <- do.call('rbind',tmp)
          sds <- do.call('rbind',tmp.sd)
          if(surv == "Winter")
          {
            tmp2 <- do.call('rbind',tmp2)
            tmp2.sd <- do.call('rbind',tmp2.sd)
            prob <- bind_rows(tmp2,prob) # Only needed for the Winter survey
            sds <- bind_rows(tmp2.sd,sds)
          } # end if(surv == "Winter")
          prob$area <- prob %>% st_area() %>% set_units("km^2") # recalculate area based on polygons...

        } # end if(spec == "Cod")
        
        
        # Now do the same thing for Yellowtail...
        if(spec == "Yellowtail")
        {
           # This won't work for years in which the random field isn't 3 years, i.e. the first year in the yellowtail in spring. Winter and Fall happen to be fine for YT :-)
          if(surv == "Spring")
          {
            prob.first <-   prob.tmp %>% dplyr::filter(yrs == "1970-71") 
            prob.tmp   <-   prob.tmp %>% dplyr::filter(yrs != "1970-71")
            sds.first  <-   sds.tmp %>% dplyr::filter(yrs == "1970-71") 
            sds.tmp    <-   sds.tmp %>% dplyr::filter(yrs != "1970-71")
          } # End surv == spring
          
          tmp <- NULL
          tmp2 <- NULL
          tmp.sd <- NULL
          tmp2.sd <- NULL
          ####For the fall we have a 5 year loop, and all era's are 5 years long so easy peasy...
          if(surv == "Fall") 
          {
            for(p in 0:4) 
            {
              tmp[[p+1]] <- prob.tmp %>% dplyr::mutate(year = as.numeric(first_year) + p)
              tmp.sd[[p+1]] <- sds.tmp %>% dplyr::mutate(year = as.numeric(first_year) + p)
            } #end for loop
          } # end if fall statement
          
          # For the Winter and Spring surveys we have a 3 year field...
          if(surv != "Fall") 
          {  
            for(p in 0:2)
            {
              tmp[[p+1]] <- prob.tmp %>% dplyr::mutate(year = as.numeric(first_year) + p)
              tmp.sd[[p+1]] <- sds.tmp %>% dplyr::mutate(year = as.numeric(first_year) + p)
              if(surv == "Spring" & p < 2)
              {
                tmp2[[p+1]] <- prob.first %>% dplyr::mutate(year = as.numeric(first_year) + p) # Only needed for the Spring survey
                tmp2.sd[[p+1]] <- sds.first %>% dplyr::mutate(year = as.numeric(first_year) + p) # Only needed for the Spring survey
              }  
            } # end p loop
          } # end surv fall statement
          # Unwrap and combine lists
          prob <- do.call('rbind',tmp)
          sds <- do.call('rbind',tmp.sd)
          if(surv == "Spring")
          {
            tmp2 <- do.call('rbind',tmp2)
            tmp2.sd <- do.call('rbind',tmp2.sd)
            prob <- bind_rows(tmp2,prob) 
            sds <- bind_rows(tmp2.sd,sds)
          } # End spring
          prob$area <- prob %>% st_area() %>% set_units("km^2") # recalculate area based on polygons...
      } # end if(spec == "Yellowtail")
      } # end if statement handling the areas that aren't the cod and yt closures.
      
      # Now to the Cod and Yellowtail closures
      if(nm %in% c("Cod closure","Yellowtail closure"))
      {
        tmp <- NULL
        tmp.sd <- NULL
        for(p in 1:nrow(area))
        {
          year.to.pick <- first.years[max(which(first.years <= area$year[p]))]
          tmp.yrs <- (pred.sub %>% dplyr::filter(first_year == year.to.pick) %>% dplyr::select(yrs))$yrs[1]
          tmp[[p]] <- st_intersection(preds.prob %>% dplyr::filter(first_year == year.to.pick),area %>% dplyr::filter(year ==area$year[p]))
          tmp.sd[[p]] <- st_intersection(pred.sub %>% dplyr::filter(first_year == year.to.pick),area %>% dplyr::filter(year ==area$year[p]))
          if(nrow(tmp[[p]]) ==0) 
          {
            tmp[[p]] <- tmp.sd[[p]][1,] # Just grab the first row from tmp.sd as we know we'll have data there, then delete the data and fill it with NAs....
            tmp[[p]][names(tmp[[p]]) %in% names(tmp[[p]])] <- NA
            # A FAKE polygon that is nowhere near anywhere...
            st_geometry(tmp[[p]]) <- st_sfc(st_polygon(list(matrix(c(0,0,0.1,0,0.1,0.1,0,0.1,0,0),ncol=2, byrow=TRUE))),crs=st_crs(preds.prob)) 
            tmp[[p]]$survey <- surv
            tmp[[p]]$species <- spec
            tmp[[p]]$yrs <- tmp.yrs
          } # end if statement
          tmp[[p]]$year <- tmp.sd[[p]][1,]$year.1
          tmp.sd[[p]]$year <- tmp.sd[[p]]$year.1
        } # end for p loop
        prob <- do.call('rbind',tmp)
        sds <- do.call('rbind',tmp.sd)
        # Clean up
        prob <- prob %>% dplyr::select(!c("Group_1","Document","year.1","StartDay","StartMonth","LastDay","LastMonth","PID","Species"))
        prob$area <- prob %>% st_area() %>% set_units("km^2")
        sds <- sds %>% dplyr::select(!c("Group_1","Document","year.1","StartDay","StartMonth","LastDay","LastMonth","PID","Species"))
        #all.years <- data.frame(yrs = unique(sds$yrs)) 
        #first.years <- as.numeric(substr(all.years$yrs,1,4))
      } # end  if(names(areas[k]) %in% c("Cod closure","Yellowtail closure"))
      
      # Now stitch everything back together again..
       # First summarize the SD data
      tmp.sd <- sds %>% data.frame() %>% dplyr::group_by(year,yrs,.drop=F) %>%  dplyr::summarize(mn.stan.sd.dist = mean(stan.sd.dist),
                                                                                                 med.stan.sd.dist = median(stan.sd.dist),
                                                                                                 mn.sd.dist = mean(sd.dist),
                                                                                                 med.sd.dist = median(sd.dist),
                                                                                                 mn.pred = mean(pred),
                                                                                                 med.pred = median(pred),
                                                                                                 mn.sd = mean(pred.sd),
                                                                                                 med.sd = median(pred.sd))
      tmp.sd$loc <- nm
      tmp.sd$species <- spec
      tmp.sd$survey <- surv
      sd.summary[[paste(surv,spec,nm,sep="_")]]  <- tmp.sd
      
      # Now the area calcs
      tmp.area <- prob  %>% dplyr::group_by(year,yrs) %>% dplyr::summarize(tot.area = sum(area))
      st_geometry(tmp.area) <- NULL # Strip out the geometry...
      # If there are 0's in the data when we want to have data we are going to have to fill those with 0's now....
      if(!nm %in% c("Cod closure","Yellowtail closure")) # But this has been done for the cod/yt closures, it's the other areas we need to do something fun.
      {
        year.range <- data.frame(year = seq(min(tmp.sd$year),max(tmp.sd$year),by=1))
        tmp.area <- left_join(year.range,tmp.area,by='year')
        tmp.area$tot.area[is.na(tmp.area$tot.area)] <- 0
        tmp.area$yrs <- tmp.sd$yrs
      }
      tmp.area <- tmp.area[order(tmp.area$year),]
      
      tmp.area$loc <- nm
      tmp.area$species <- spec
      tmp.area$survey <- surv
      area.summary[[paste(surv,spec,nm,sep="_")]]  <- tmp.area
     
  } # end for(k in 1:n.areas)
    }} # End i, j loops
    
  
  
  
# Unwrap the sd and area lists into tidy dataframes.
sd.all <- do.call('rbind',sd.summary)
area.all <- do.call('rbind',area.summary)
# Make very small values go to 0...
area.all$tot.area <- as.numeric(area.all$tot.area) # Strip off the km^2 from the total area
area.all$tot.area[area.all$tot.area < 0.1] <- 0 # and make anything less that 0.1 km^2 = 0 (mostly effects Cod and YT closure hack I did)
# Would be nice to get a 'proportion of closure that is core area.  So we need to stick the size of the areas onto the area.all object
area.all$clos.size <- NA
for(i in 1:n.areas)
{
  nm <-names(areas)[i]
  #tmp <- area.all %>% dplyr::filter(loc == names(areas)[i])
  # Now that's ugly!
  if(!nm %in% c("Cod closure","Yellowtail closure"))
  {
  clos.size <- areas[[i]] %>% st_area() %>% set_units("km^2") %>% 
                                  as.numeric() %>% as.data.frame() %>% dplyr::summarize(sum = sum(.)) %>% as.numeric()
  area.all$clos.size[area.all$loc == nm] <- clos.size
  }# End the static closures

  # Now the dynamic closures need another annual loop annoyingly...
  if(nm %in% c("Cod closure","Yellowtail closure"))
  {
    years <- unique(tmp$year)
    n.years <- length(years)
    for(j in years)
    {
      clos.size <- areas[[i]] %>% dplyr::filter(year == j) %>% st_area() %>% set_units("km^2") %>% as.numeric() 
      area.all$clos.size[area.all$loc == nm & area.all$year == j] <- clos.size
    } # End loop through the years...
  }# end the dynamic closures...
  
}# End the i loop to get areas on all the closures...

# Now calculate the proportion of closed area that is 'Core".
area.all <- area.all %>% dplyr::mutate(prop = tot.area/clos.size)
area.all$loc[area.all$loc == "Closed Area 2"] <- "Closed Area II"
sd.all$loc[sd.all$loc == "Closed Area 2"] <- "Closed Area II"
area.all$loc[area.all$loc == "Closed Area 1"] <- "Closed Area I"
sd.all$loc[sd.all$loc == "Closed Area 1"] <- "Closed Area I"

# And here we'll split the object into the closures and the larger regional summaries
area.regions <- area.all %>% dplyr::filter(loc %in% c("Canada","US", "Georges Bank"))
area.closures <- area.all %>% dplyr::filter(loc %in% c("Closed Area I","Closed Area II","Groundfish closure","Cod closure","Yellowtail closure"))
area.closures$loc <-  factor(area.closures$loc,levels = c("Closed Area I","Closed Area II","Groundfish closure","Cod closure","Yellowtail closure"))
sd.regions <- sd.all %>% dplyr::filter(loc %in% c("Canada","US", "Georges Bank"))
sd.closures <- sd.all %>% dplyr::filter(loc %in% c("Closed Area I","Closed Area II","Groundfish closure","Cod closure","Yellowtail closure"))
sd.closures$loc <-  factor(sd.closures$loc,levels = c("Closed Area I","Closed Area II","Groundfish closure","Cod closure","Yellowtail closure"))

#save(list = c("sd.all","area.all"),file = paste0(direct.proj,"Results/Core_area_and_SD_calcs.RData"))
#load(paste0(direct.proj,"Results/Core_area_and_SD_calcs.RData"))

# Colors of the survey
surv.cols <- c("black", "blue","darkgreen")
# The CAI and CAII dates are December 1994 as per Murawski 2000 paper, the Groundfish closure was first put in place in 1970 as per Halliday 1988
strt.years <- data.frame(year = c(rep(1994,4),rep(1970,2),rep(2006,2),rep(2007,2)),
                         loc = c(rep("Closed Area I",2),rep("Closed Area II",2),rep("Groundfish closure",2),rep("Cod closure",2),rep("Yellowtail closure",2)))
strt.years$loc <-  factor(strt.years$loc,levels = c("Closed Area I","Closed Area II","Groundfish closure","Cod closure","Yellowtail closure"))

# So the plots of the Area time series...
plt.core.area <- ggplot(area.closures) + 
                           geom_line(aes(y = tot.area, x= year,color=survey),size=1)  + facet_wrap(~loc + species,scales = 'free_y',ncol=2) +
                           geom_line(aes(y =clos.size ,x=year),size=0.5,linetype ='dashed') +
                           geom_vline(data=strt.years,aes(xintercept = year),col='grey',linetype='dotted',size=1) +
                           scale_color_manual(values = surv.cols) + scale_x_continuous(name ="",breaks=seq(1970,2020,by=5)) +
                           theme(legend.title = element_blank()) + ylab("Core area (km²) in Closure")

save_plot(paste0(direct.proj,"Results/Figures/Core_area_closure.png"),plt.core.area,base_width =11,base_height =11,units='in')
# Plot of the proportion of core area in each closure over time
plt.prop.core <- ggplot(area.closures) + 
                           geom_line(aes(y = prop, x= year,color=survey),size=1)  + facet_wrap(~loc + species,ncol=2) + scale_y_continuous(limits = c(0,1.01)) +
                           geom_vline(data=strt.years,aes(xintercept = year),col='grey',linetype='dotted',size=1) +
                           scale_color_manual(values = surv.cols) + scale_x_continuous(name ="",breaks=seq(1970,2020,by=5)) +
                           theme(legend.title = element_blank()) + ylab("Proportion of Closure that is Core Area")

save_plot(paste0(direct.proj,"Results/Figures/Core_prop_closure.png"),plt.prop.core,base_width =11,base_height =11,units='in')
# Plot of the mean EP changes in each closure over time.
plt.pred <- ggplot(sd.closures) + 
                           geom_line(aes(y = mn.pred, x= year,color=survey),size=1)  + facet_wrap(~loc + species,ncol=2) + scale_y_continuous(limits = c(0,1)) +
                           geom_vline(data=strt.years,aes(xintercept = year),col='grey',linetype='dotted',size=1) + 
                           scale_color_manual(values = surv.cols) + scale_x_continuous(name ="",breaks=seq(1970,2020,by=5)) +
                           theme(legend.title = element_blank()) + ylab("Mean Encounter Probability (EP)")

save_plot(paste0(direct.proj,"Results/Figures/EP_closure.png"),plt.pred,base_width =11,base_height =11,units='in')


sd.range.stan.dist <- range(sd.closures$mn.stan.sd.dist)        
plt.stan.sd.dist <- ggplot(sd.closures) + 
                           geom_line(aes(y = mn.stan.sd.dist, x= year,color=survey),size=1)  + facet_wrap(~loc + species,ncol=2)  +
                           scale_y_continuous(limits = c(min(sd.range.stan.dist),max(sd.range.stan.dist))) +
                           geom_vline(data=strt.years,aes(xintercept = year),col='grey',linetype='dotted',size=1) +                         
                           geom_hline(yintercept = 0,col='grey',size=0.5,linetype = 'dashed') +
                           scale_color_manual(values = surv.cols) + scale_x_continuous(name ="",breaks=seq(1970,2020,by=5)) +
                           theme(legend.title = element_blank()) + ylab("Standardized Mean SD distance")
save_plot(paste0(direct.proj,"Results/Figures/stan_sd_dist_closure.png"),plt.stan.sd.dist,base_width =11,base_height =11,units='in')



sd.range.dist <- range(sd.closures$mn.sd.dist)    
plt.sd.dist <- ggplot(sd.closures) + 
                           geom_line(aes(y = mn.sd.dist, x= year,color=survey),size=1)  + facet_wrap(~loc + species,ncol=2) +
                           scale_y_continuous(limits = c(min(sd.range.dist),max(sd.range.dist))) +
                           geom_vline(data=strt.years,aes(xintercept = year),col='grey',linetype='dotted',size=1) +                         
                           geom_hline(yintercept = 0,col='grey',size=0.5,linetype = 'dashed') +
                           scale_color_manual(values = surv.cols) + scale_x_continuous(name ="",breaks=seq(1970,2020,by=5)) +
                           theme(legend.title = element_blank()) + ylab("Mean SD distance")
save_plot(paste0(direct.proj,"Results/Figures/sd_dist_closure.png"),plt.sd.dist,base_width =11,base_height =11,units='in')


sd.range <- range(sd.closures$mn.sd)
plt.sd <- ggplot(sd.closures) + 
                           geom_line(aes(y =mn.sd, x= year,color=survey),size=1)  +  facet_wrap(~loc + species,ncol=2) + 
                           geom_vline(data=strt.years,aes(xintercept = year),col='grey',linetype='dotted',size=1) +
                           scale_y_continuous(limits = c(min(sd.range),max(sd.range))) +
                           scale_color_manual(values = surv.cols) + scale_x_continuous(name ="",breaks=seq(1970,2020,by=5)) +
                           theme(legend.title = element_blank()) + ylab("Mean Standard Deviation")
save_plot(paste0(direct.proj,"Results/Figures/sd_closure.png"),plt.sd,base_width =11,base_height =11,units='in')

# These plots should stick with using all the data...
plt.pred.sd <- ggplot(pred.res) + geom_point(aes(y= pred,x=pred.sd,color = survey)) 
plot.pred.sd.dist <-  ggplot(sd.all) + geom_point(aes(y= mn.pred,x=mn.sd.dist,color=survey))



      # here would be a sample plot for CA1 with the sd data...

# Looking at our metric spatially, seems like it is reasonable enough to move forward. 
hgt <- unit(5,'cm')
brk <- seq(-0.2,0.4,by=0.05)
lims <- range(brk)
col <- addalpha(pals::viridis(101),1)
sf <- scale_fill_gradientn(colours = col, limits=lims,breaks=brk,name=paste0(en2fr("Deviation from sd",french,case='sentence',custom = rosetta_terms)))
sc <- scale_colour_gradientn(colours = col, limits=lims,breaks=brk,name=paste0(en2fr("Deviation from sd",french,case='sentence',custom = rosetta_terms)))

bp.gb <- pecjector(area="GB",plot=F,repo = 'github',
           add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),
           add_custom= list(obj = CA1),
           c_sys = 32619,buffer = 0.2) + theme_map()


plt.dev.sd.tst <- bp.gb + geom_sf(data = cls.sub  ,aes(fill = stan.sd.dist,colour=stan.sd.dist)) +
facet_wrap(~yrs,ncol=6) +
  coord_sf(datum=32619) + sf + sc  +# theme_map() +
 theme(legend.key.width =hgt,text = element_text(size=16), legend.position="top")
      
      
      

      
      
    
   
  
  
    

    
 
 ```
 


```{r overveiw-fig, echo=F, include=F, cache =T}
################################
###  Figures for the paper  ###
################################

# Plane old map
bp <-  pecjector(area="GOM",plot=F,repo = 'github',add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),c_sys = 32619,buffer = 0.2) + theme_map()

# Lets get a basemap set up for the rest of the show.
bp.zoom <- pecjector(c_sys = 32619,area = list(x=c(580000,780000), y = c(4530000,4680000),crs = 32619),add_layer = list(land='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),plot=F) #+ theme_map()

# Same but with bathy underlain
bp.bathy <-  pecjector(area="GOM",plot=F,repo = 'github',add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl',bathy = c(10,'s',200)),c_sys = 32619,buffer = 0.2) + theme_map()


# Maybe I want this sometime.
# bp.closures <- bp.bathy + geom_sf(data= CA1,fill = NA,color = 'red',size=1) + 
#                           geom_sf(data= CA2,fill = NA,color = 'green',size=1) +
#                           geom_sf(data= yt.closures, fill= NA,color = 'blue',size=1) + 
#                           geom_sf(data=cod.closures, fill= NA,color = 'black',size=1) 
# A figure providing a general overview of the area....

# #  A nice clean polygon of the core of the GB area we want to deal with here.
 clp.poly <- st_as_sf(data.frame(X = c(508000,508000,900000,650000,600000,550000),
                                 Y=c(4540000,4350000,4674000,4674000,4661000,4622000),ID=1),coords = c("X","Y"),crs= 32619)
 clp.poly <- st_cast(st_combine(clp.poly),"POLYGON")
# # Now use the bigger clp with this other clip to get a nice clipped GB area...
 clp.pred <- st_intersection(clp,clp.poly)

#####
# Figure Overview of the area, don't need to run this every time, if I want to change something uncomment the below
#####
labs <- st_as_sf(data.frame(X =c(600000, 680000), Y = c(4700000,4700000),lab = c("U.S.","Canada")),coords = c("X","Y"),crs=32619)
 
plt.over <- bp.bathy + geom_sf(data = clp.pred,fill=NA,size = 1.5, color='orange') + 
                       geom_sf(data= dat.sf,alpha = 0.25,shape=19,size=0.25, fill='black',color='black')+
                       #geom_sf(data = CA1,fill = NA,size=1.25,color='blue') + geom_sf(data = CA2,fill = NA,size=1.25,color = 'white') + 
                       #geom_sf(data = yt.closures,size=0.5,fill = 'lightgrey', alpha = 0.1,color = 'gold') + 
                       #geom_sf(data = cod.closures,size=0.5,fill = 'lightgrey', alpha = 0.1,color = 'gold') + 
                       geom_sf_label(data = labs,aes(label = lab),parse=T)  
   
 
save_plot(paste0(direct.proj,"Results/Figures/GB_overview.png"),plt.over,base_width =6,base_height =8,units='in',dpi=300)
save_plot(paste0(direct.proj,"Results/Figures/GB_overview.tiff"),plt.over,base_width =6,base_height =8,units='in',dpi=300)
#over.plt <- paste0(direct.proj,"Results/Figures/GB_overview.png")
```


```{r mesh-plt, echo=F, include=F, paged.print=FALSE,cache =T}

#####
# Figure MESH don't need to run this every time, if I want to change something uncomment the below
#####

# I'm gonna need to plot my mesh on the nice bp object
mesh.gf$crs <- crs("+init=epsg:32619") 
# THis is a very minor tweak on a custom function from Finn Lindgren https://groups.google.com/forum/#!topic/r-inla-discussion-group/z1n1exlZrKM
mesh.sf <- inla.mesh2sf(mesh.gf)

plt.mesh <- pecjector(area = "GOM",buffer=0.4, c_sys = 32619,plot=F,
                     add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),
                     add_custom = list(obj = mesh.sf$triangles, size=0.5,color= 'grey30')) +
                     theme_map()
save_plot(paste0(direct.proj,"Results/Figures/mesh.tiff"),plt.mesh,base_width =8,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/mesh.png"),plt.mesh,base_width =8,base_height =8,units='in')
#mesh.plt <- paste0(direct.proj,"Results/Figures/mesh.png")
```

```{r mesh-grid-plt, echo=F, include=F, paged.print=FALSE,cache=T}
####
# Figure for the mesh grid
#####
plt.mesh.grid <- pecjector(area = list(x=c(540000,780000), y = c(4500000,4680000),crs = 32619),buffer=0.4, c_sys = 32619,plot=F,
                     add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl',bathy = c(50,'s')),
                     add_custom = list(obj = mesh.grid, size=0.5,color= 'grey30')) +
                     theme_map()
save_plot(paste0(direct.proj,"Results/Figures/mesh_grid.tiff"),plt.mesh.grid,base_width =8,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/mesh.grid.png"),plt.mesh.grid,base_width =8,base_height =8,units='in')

```

```{r ,figs-load, echo=F, include=F, paged.print=FALSE,cache =T}
# Here we load the figures we need for later, saves lots of running above if nothing has changed...
over.plt <- paste0(direct.proj,"Results/Figures/GB_overview.png") # Overview plot
mesh.plt <- paste0(direct.proj,"Results/Figures/mesh.png") # Mesh plot
mesh.grid.plt <- paste0(direct.proj,"Results/Figures/mesh.grid.png")



```

```{r inline-data-for-paper, echo=F, include=F, paged.print=FALSE,cache =F}

################################
###  Getting data for use in the paper  ###
################################
n.stations <- dat.final %>% group_by(survey) %>% dplyr::summarise(ns = n())
n.nmfs.spring <- n.stations$ns[n.stations$survey == 'nmfs-spring']
n.rv <- n.stations$ns[n.stations$survey == 'RV']
n.nmfs.fall <- n.stations$ns[n.stations$survey == 'nmfs-fall']

# What size are the mesh grid cells
mesh.grid.size <- st_area(mesh.grid) %>% set_units("km^2") %>% as.numeric() %>% mean() %>% round(digits=1)

# Sediment type overall...
sed.bd <- table(dat.final$SEDNUM)
per.3.4.sed <- signif(100*sum(sed.bd[2:3])/length(dat.final$SEDNUM), digits = 2)
# Other sediment types are 5.7% of tows, along with a small number of NA tows (which table doesn't show).
per.other.sed <- signif(100*sum(sed.bd[-c(2:3)])/length(dat.final$SEDNUM), digits = 2)
# Get some of the cod FE numbers... hard to pick a number here give variabilty but save to say looking at these the drop occurs between 10 and 11 °C
wt.cod <- cod.fe.res %>% dplyr::filter(survey == "Winter" & fe == "SST") 
st.cod <- cod.fe.res %>% dplyr::filter(survey == "Spring" & fe == "SST") 
ft.cod <- cod.fe.res %>% dplyr::filter(survey == "Fall" & fe == "SST") 

peaks <- aggregate(response ~ survey + fe,data = cod.fe.res, FUN = function(x) which(x == max(x)))
wd.cod <- cod.fe.res %>% dplyr::filter(survey == "Winter" & fe == "Depth") 
sd.cod <- cod.fe.res %>% dplyr::filter(survey == "Spring" & fe == "Depth") 
s.dep.peak <- sd.cod$covar[peaks$response[2]]
w.dep.peak <- wd.cod$covar[peaks$response[1]]
c.dep.peak <- paste0(round(s.dep.peak,digits =0),"-",round(w.dep.peak,digits=0))

# For yellowtail pick a peak depth as well
peaks <- aggregate(response ~ survey + fe,data = yt.fe.res, FUN = function(x) which(x == max(x)))
wd.yt <- yt.fe.res %>% dplyr::filter(survey == "Winter" & fe == "Depth") 
sd.yt <- yt.fe.res %>% dplyr::filter(survey == "Spring" & fe == "Depth") 
fd.yt <- yt.fe.res %>% dplyr::filter(survey == "Fall" & fe == "Depth") 
s.dep.peak <- as.numeric(sd.yt$covar[peaks$response[2]])
w.dep.peak <- as.numeric(wd.yt$covar[peaks$response[1]])
f.dep.peak <- as.numeric(fd.yt$covar[peaks$response[3]])
# Spring and winter are the deepest so take those..
yt.dep.peak <- paste0(round(s.dep.peak,digits =0),"-",round(w.dep.peak,digits=0))

# Hyperparamters
range.winter.cod.est <- hyper.mod.est %>% dplyr::filter(model == "cod.winter" , hyper == "Range for w")
range.winter.cod.mn <- signif(range.winter.cod.est$mean/1000,digits =3)
range.winter.cod.lci <- signif(range.winter.cod.est$`0.025quant`/1000,digits =2)
range.winter.cod.uci <- signif(range.winter.cod.est$`0.975quant`/1000,digits =3)
range.winter.cod.mn.ci <- paste0(range.winter.cod.mn," (95% CI:",range.winter.cod.lci,"-",range.winter.cod.uci,")")

range.spring.cod.est <- hyper.mod.est %>% dplyr::filter(model == "cod.spring" , hyper == "Range for w")
range.spring.cod.mn <- signif(range.spring.cod.est$mean/1000,digits =3)
range.spring.cod.lci <- signif(range.spring.cod.est$`0.025quant`/1000,digits =3)
range.spring.cod.uci <- signif(range.spring.cod.est$`0.975quant`/1000,digits =3)
range.spring.cod.mn.ci <- paste0(range.spring.cod.mn," (95% CI:",range.spring.cod.lci,"-",range.spring.cod.uci,")")

range.winter.yt.est <- hyper.mod.est %>% dplyr::filter(model == "yt.winter" , hyper == "Range for w")
range.winter.yt.mn <- signif(range.winter.yt.est$mean/1000,digits =2)
range.winter.yt.lci <- signif(range.winter.yt.est$`0.025quant`/1000,digits =2)
range.winter.yt.uci <- signif(range.winter.yt.est$`0.975quant`/1000,digits =3)
range.winter.yt.mn.ci <- paste0(range.winter.yt.mn," (95% CI:",range.winter.yt.lci,"-",range.winter.yt.uci,")")

```
